{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmvIYBpwZJ5X",
        "outputId": "21e55f7f-1777-4b76-d995-93b9ec4d905a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMNo3VLNhDAU",
        "outputId": "47350291-21e8-4cf7-e141-ffeeac85d4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.4 [186 kB]\n",
            "Fetched 186 kB in 1s (281 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "\n",
        "# Vérifie si le dossier de sortie existe, sinon le crée\n",
        "if not os.path.exists(\"extracted_images\"):\n",
        "    os.mkdir(\"extracted_images\")\n",
        "\n",
        "# Chemin du répertoire contenant les fichiers PDF\n",
        "pdf_directory = \"/content\"\n",
        "\n",
        "# Obtient la liste de tous les fichiers dans le répertoire\n",
        "files = os.listdir(pdf_directory)\n",
        "\n",
        "# Filtre pour ne garder que les fichiers PDF\n",
        "pdf_files = [file for file in files if file.endswith(\".pdf\")]\n",
        "\n",
        "# Boucle sur chaque fichier PDF\n",
        "for pdf_file in pdf_files:\n",
        "    # Chemin complet du fichier PDF\n",
        "    pdf_path = os.path.join(pdf_directory, pdf_file)\n",
        "\n",
        "    # Convertit la première page du PDF en image JPEG\n",
        "    pages = convert_from_path(pdf_path, first_page=1, last_page=1)\n",
        "\n",
        "    # Enregistre l'image convertie\n",
        "    if pages:\n",
        "        # Nom du fichier pour l'image de la première page\n",
        "        image_name = os.path.splitext(pdf_file)[0] + \"_first_page.jpg\"\n",
        "        # Enregistre l'image dans le dossier de sortie\n",
        "        pages[0].save(os.path.join(\"extracted_images\", image_name), \"JPEG\")\n",
        "        print(f\"Image de la première page de {pdf_file} extraite avec succès.\")\n",
        "    else:\n",
        "        print(f\"Aucune page trouvée dans le document {pdf_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdog5qzxanpT",
        "outputId": "e09e5f03-2f1f-47b8-f323-31ca820e8e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image de la première page de article pdf perspectives médiévales signaler le mensonge.pdf extraite avec succès.\n",
            "Image de la première page de Préface Europe.pdf extraite avec succès.\n",
            "Image de la première page de REY-Culture&Musées.pdf extraite avec succès.\n",
            "Image de la première page de BeMiChaGa22.pdf extraite avec succès.\n",
            "Image de la première page de Sécurité des objets connectés _ attaquer pour mieux se défendre_1_.pdf extraite avec succès.\n",
            "Image de la première page de grabar-AFIA2021-112.pdf extraite avec succès.\n",
            "Image de la première page de Temporary Event Urbanism.pdf extraite avec succès.\n",
            "Image de la première page de Eclairs analogiques.pdf extraite avec succès.\n",
            "Image de la première page de VergerXXV - MathildeMougin.pdf extraite avec succès.\n",
            "Image de la première page de 2020_Zenetti_Farge.pdf extraite avec succès.\n",
            "Image de la première page de balisages_2_5_bertrand_degrange.pdf extraite avec succès.\n",
            "Image de la première page de De l Ontologie philosophique aux ontologies informatiques.pdf extraite avec succès.\n",
            "Image de la première page de 57c4073da94c4d6ec4afe7ca1b86a341.pdf extraite avec succès.\n",
            "Image de la première page de reférentiel SI.pdf extraite avec succès.\n",
            "Image de la première page de Bouchabou2023RODA.pdf extraite avec succès.\n",
            "Image de la première page de theconversation.com-Le machine learning nouvelle porte dentrée pour lesnbspattaquants dobjetsnbspconnectés.pdf extraite avec succès.\n",
            "Image de la première page de tipa-5424.pdf extraite avec succès.\n",
            "Image de la première page de 346_Ruggia_Vanni_HAL.pdf extraite avec succès.\n",
            "Image de la première page de CR évènement.pdf extraite avec succès.\n",
            "Image de la première page de TAL_corrections.pdf extraite avec succès.\n",
            "Image de la première page de L’Enseignement du FLE en Face à Face et En Ligne _ deux modes d'intervention pédagogiques complémentaires.pdf extraite avec succès.\n",
            "Image de la première page de I.KOR CHAHINE_2020_De l'animalisation à la neutralisation_pdf.pdf extraite avec succès.\n",
            "Image de la première page de Text_integralle_et_Poster.pdf extraite avec succès.\n",
            "Image de la première page de Aspect_Extraction__CRI_2019.pdf extraite avec succès.\n",
            "Image de la première page de 46. Nerval, Boleslas Ier, Chrobry le Grand.pdf extraite avec succès.\n",
            "Image de la première page de ZAOUAQ_2020.pdf extraite avec succès.\n",
            "Image de la première page de amue-collection-numerique-29_p56-57.pdf extraite avec succès.\n",
            "Image de la première page de gautier2023j3ea.pdf extraite avec succès.\n",
            "Image de la première page de Créativité assistée par ordinateur composer la musique d'un film final HAL.pdf extraite avec succès.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tesserocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BZ7_FJxddWs",
        "outputId": "41fe2916-8402-4ac8-b304-329e59e3a0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tesserocr\n",
            "  Downloading tesserocr-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tesserocr\n",
            "Successfully installed tesserocr-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tesseract-ocr/tessdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0dxpdX6oVqb",
        "outputId": "6acfdf87-bb7d-464a-a331-373b656d7eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tessdata' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV0RRrbKoWRD",
        "outputId": "875b40ad-de1b-4fb5-99f0-7e89ac9bcd52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Chemin du répertoire contenant les images\n",
        "image_directory = \"/content/extracted_images\"  # Remplacez \"/content/images\" par le chemin de votre répertoire\n",
        "\n",
        "# Chemin du répertoire de sortie pour les images binarisées\n",
        "output_directory = \"/content/binarized_images\"  # Chemin du répertoire de sortie\n",
        "\n",
        "# Vérifie si le dossier de sortie existe, sinon le crée\n",
        "if not os.path.exists(output_directory):\n",
        "    os.mkdir(output_directory)\n",
        "\n",
        "# Obtient la liste de tous les fichiers dans le répertoire\n",
        "files = os.listdir(image_directory)\n",
        "\n",
        "# Filtre pour ne garder que les fichiers image\n",
        "image_files = [file for file in files if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\"))]\n",
        "\n",
        "# Boucle sur chaque fichier image\n",
        "for image_file in image_files:\n",
        "    # Chemin complet du fichier image d'entrée\n",
        "    image_path = os.path.join(image_directory, image_file)\n",
        "\n",
        "    # Chargez l'image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Convertir en niveaux de gris\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Binariser l'image\n",
        "    _, binarized_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Chemin complet du fichier image de sortie\n",
        "    output_path = os.path.join(output_directory, \"binarized_\" + image_file)\n",
        "\n",
        "    # Enregistrer l'image binarisée\n",
        "    cv2.imwrite(output_path, binarized_image)\n",
        "\n",
        "    print(f\"Image binarisée enregistrée avec succès: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJxv2TjupLMd",
        "outputId": "c325c1d7-2abb-4e80-82ff-6f5843c6ca7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_Sécurité des objets connectés _ attaquer pour mieux se défendre_1__first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_gautier2023j3ea_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_46. Nerval, Boleslas Ier, Chrobry le Grand_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_reférentiel SI_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_Créativité assistée par ordinateur composer la musique d'un film final HAL_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_Bouchabou2023RODA_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_VergerXXV - MathildeMougin_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_De l Ontologie philosophique aux ontologies informatiques_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_346_Ruggia_Vanni_HAL_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_grabar-AFIA2021-112_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_Aspect_Extraction__CRI_2019_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_2020_Zenetti_Farge_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_TAL_corrections_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_Eclairs analogiques_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_REY-Culture&Musées_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_BeMiChaGa22_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_L’Enseignement du FLE en Face à Face et En Ligne _ deux modes d'intervention pédagogiques complémentaires_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_I.KOR CHAHINE_2020_De l'animalisation à la neutralisation_pdf_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_Text_integralle_et_Poster_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_theconversation.com-Le machine learning nouvelle porte dentrée pour lesnbspattaquants dobjetsnbspconnectés_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_Préface Europe_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_amue-collection-numerique-29_p56-57_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_57c4073da94c4d6ec4afe7ca1b86a341_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_ZAOUAQ_2020_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_CR évènement_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_tipa-5424_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_article pdf perspectives médiévales signaler le mensonge_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_balisages_2_5_bertrand_degrange_first_page.jpg\n",
            "Image binarisée enregistrée avec succès: /content/binarized_images/binarized_Temporary Event Urbanism_first_page.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tesserocr import PyTessBaseAPI\n",
        "import os\n",
        "\n",
        "# Chemin du répertoire contenant les images\n",
        "image_directory = \"/content/binarized_images\"\n",
        "# Chemin du répertoire de sortie pour les résultats OCR\n",
        "output_directory = \"/content/ocr_results\"\n",
        "\n",
        "# Vérifie si le dossier de sortie existe, sinon le crée\n",
        "if not os.path.exists(output_directory):\n",
        "    os.mkdir(output_directory)\n",
        "\n",
        "# Obtient la liste de tous les fichiers dans le répertoire\n",
        "files = os.listdir(image_directory)\n",
        "\n",
        "# Filtre pour ne garder que les fichiers image\n",
        "image_files = [file for file in files if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\"))]\n",
        "\n",
        "# Boucle sur chaque fichier image\n",
        "for image_file in image_files:\n",
        "    # Chemin complet du fichier image\n",
        "    image_path = os.path.join(image_directory, image_file)\n",
        "\n",
        "    # Initialiser l'API Tesseract OCR\n",
        "    with PyTessBaseAPI(path='/content/tessdata') as api:\n",
        "        # Définir l'image pour l'analyse OCR\n",
        "        api.SetImageFile(image_path)\n",
        "\n",
        "        # Obtenir le texte OCR\n",
        "        ocr_text = api.GetUTF8Text()\n",
        "\n",
        "        # Enregistrer le résultat OCR dans un fichier texte\n",
        "        output_file = os.path.join(output_directory, os.path.splitext(image_file)[0] + \".txt\")\n",
        "        with open(output_file, \"w\") as f:\n",
        "            f.write(ocr_text)\n",
        "\n",
        "        print(f\"Résultat OCR enregistré avec succès pour {image_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv9GS7UM4nBM",
        "outputId": "568c6c30-182e-4041-de8c-63c81c479d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Résultat OCR enregistré avec succès pour binarized_article pdf perspectives médiévales signaler le mensonge_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_346_Ruggia_Vanni_HAL_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_CR évènement_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_L’Enseignement du FLE en Face à Face et En Ligne _ deux modes d'intervention pédagogiques complémentaires_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_VergerXXV - MathildeMougin_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_ZAOUAQ_2020_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_balisages_2_5_bertrand_degrange_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_I.KOR CHAHINE_2020_De l'animalisation à la neutralisation_pdf_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_tipa-5424_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_Bouchabou2023RODA_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_gautier2023j3ea_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_grabar-AFIA2021-112_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_De l Ontologie philosophique aux ontologies informatiques_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_Préface Europe_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_46. Nerval, Boleslas Ier, Chrobry le Grand_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_BeMiChaGa22_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_Sécurité des objets connectés _ attaquer pour mieux se défendre_1__first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_amue-collection-numerique-29_p56-57_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_reférentiel SI_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_Temporary Event Urbanism_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_Créativité assistée par ordinateur composer la musique d'un film final HAL_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_Eclairs analogiques_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_2020_Zenetti_Farge_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_Aspect_Extraction__CRI_2019_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_TAL_corrections_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_theconversation.com-Le machine learning nouvelle porte dentrée pour lesnbspattaquants dobjetsnbspconnectés_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_Text_integralle_et_Poster_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_REY-Culture&Musées_first_page.jpg\n",
            "Résultat OCR enregistré avec succès pour binarized_57c4073da94c4d6ec4afe7ca1b86a341_first_page.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pslZxzhdA1i7",
        "outputId": "ee749f24-31f9-4465-c4d4-08acdf59a8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.1)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import spacy\n",
        "\n",
        "# Charger le modèle de langue française\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Fonction pour extraire les informations du document OCR\n",
        "def extract_document_info(text):\n",
        "    document_info = {}\n",
        "\n",
        "    # Extraire ID\n",
        "    id_match = re.search(r'HAL Id:(.+)', text)\n",
        "    if id_match:\n",
        "        document_info[\"ID\"] = id_match.group(1).strip()\n",
        "    else:\n",
        "        document_info[\"ID\"] = \"N/A\"  # ou une valeur par défaut appropriée\n",
        "\n",
        "\n",
        "     # Extraire Titre\n",
        "    titre_match = re.search(r'open science\\n\\n(.+)', text)\n",
        "    if titre_match:\n",
        "        document_info[\"Titre\"] = titre_match.group(1)\n",
        "\n",
        "        # Extraire Auteurs\n",
        "        auteurs_match = re.search(r'\\n\\n(.+?)\\n\\n', text[titre_match.end():])\n",
        "        if auteurs_match:\n",
        "            document_info[\"Auteurs\"] = auteurs_match.group(1)\n",
        "\n",
        "    # Extraire Date de Publication\n",
        "    date_match = re.search(r'Submitted on (\\d+ \\w+ \\d{4})', text)\n",
        "    if date_match:\n",
        "        document_info[\"Date de Publication\"] = date_match.group(1).strip()\n",
        "\n",
        "    # Extraire Année de Publication\n",
        "    anneepub_match = re.search(r'(\\d{4})', text)\n",
        "    if anneepub_match:\n",
        "        document_info[\"Annee de Publication\"] = anneepub_match.group(1).strip()\n",
        "\n",
        "    # Extraire Lien\n",
        "        # Extraire Lien\n",
        "    if id_match:\n",
        "        lien_match = re.search(r'\\n(.+?)\\n', text[id_match.end():])\n",
        "        if lien_match:\n",
        "            document_info[\"Lien\"] = lien_match.group(1)\n",
        "\n",
        "    return document_info\n",
        "\n",
        "# Fonction pour retourner les tokens sans ponctuation et prépositions\n",
        "def return_clean_tokens(text):\n",
        "    # Tokeniser la phrase\n",
        "    doc = nlp(text)\n",
        "    # Filtrer les tokens\n",
        "    clean_tokens = [token.text for token in doc if not token.is_punct and not token.is_stop]\n",
        "    return clean_tokens\n",
        "\n",
        "# Chemin du répertoire contenant les fichiers texte\n",
        "repertoire_documents = \"/content/ocr_results\"\n",
        "\n",
        "# Liste pour stocker les informations extraites de tous les documents\n",
        "# Liste pour stocker les informations extraites de chaque document\n",
        "liste_informations = []\n",
        "\n",
        "# Parcourir tous les fichiers dans le dossier\n",
        "for fichier in os.listdir(repertoire_documents):\n",
        "    chemin_fichier = os.path.join(repertoire_documents, fichier)\n",
        "    with open(chemin_fichier, \"r\", encoding=\"utf-8\") as fichier:\n",
        "        contenu_fichier = fichier.read()\n",
        "        document_info = extract_document_info(contenu_fichier)\n",
        "        # Vérifier si le titre est extrait avec succès\n",
        "        if \"Titre\" in document_info:\n",
        "            # Ajouter les mots-clés au dictionnaire d'informations\n",
        "            document_info[\"Mots cles\"] = return_clean_tokens(document_info[\"Titre\"])\n",
        "            liste_informations.append(document_info)\n",
        "\n",
        "# Chemin du fichier CSV de sortie\n",
        "chemin_fichier_csv = \"/content/listinfo.csv\"\n",
        "\n",
        "# Écrire les informations extraites dans un fichier CSV\n",
        "with open(chemin_fichier_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fichier_csv:\n",
        "    # Définir les noms des colonnes\n",
        "    colonnes = [\"ID\", \"Titre\", \"Auteurs\", \"Date de Publication\", \"Annee de Publication\", \"Lien\", \"Mots cles\"]\n",
        "    writer = csv.DictWriter(fichier_csv, fieldnames=colonnes)\n",
        "    # Écrire les noms des colonnes dans le fichier CSV\n",
        "    writer.writeheader()\n",
        "    # Écrire les informations extraites pour chaque document\n",
        "    for info_document in liste_informations:\n",
        "        writer.writerow(info_document)\n",
        "\n",
        "print(\"Extraction et écriture des informations terminées.\")\n"
      ],
      "metadata": {
        "id": "nADQjgdGA2XO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e7e744-d7e4-4032-fda4-8bd7c83d3591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction et écriture des informations terminées.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import spacy\n",
        "\n",
        "# Charger le modèle de langue française\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Fonction pour extraire les informations du document OCR\n",
        "def extract_document_info(text):\n",
        "    document_info = {}\n",
        "\n",
        "    # Extraire ID\n",
        "    id_match = re.search(r'HAL Id:(.+)', text)\n",
        "    if id_match:\n",
        "        document_info[\"ID\"] = id_match.group(1).strip()\n",
        "    else:\n",
        "        document_info[\"ID\"] = \"N/A\"  # ou une valeur par défaut appropriée\n",
        "\n",
        "    # Extraire Titre\n",
        "    titre_match = re.search(r'open science\\n\\n(.+)', text)\n",
        "    if titre_match:\n",
        "        document_info[\"Titre\"] = titre_match.group(1).strip()\n",
        "\n",
        "        # Extraire Auteurs\n",
        "        auteurs_match = re.search(r'\\n\\n(.+?)\\n\\n', text[titre_match.end():])\n",
        "        if auteurs_match:\n",
        "            document_info[\"Auteurs\"] = auteurs_match.group(1).strip()\n",
        "\n",
        "    # Extraire Date de Publication\n",
        "    date_match = re.search(r'Submitted on (\\d+ \\w+ \\d{4})', text)\n",
        "    if date_match:\n",
        "        document_info[\"Date de Publication\"] = date_match.group(1).strip()\n",
        "\n",
        "    # Extraire Année de Publication\n",
        "    anneepub_match = re.search(r'(\\d{4})', text)\n",
        "    if anneepub_match:\n",
        "        document_info[\"Annee de Publication\"] = anneepub_match.group(1).strip()\n",
        "\n",
        "    # Extraire Lien\n",
        "    if id_match:\n",
        "        lien_match = re.search(r'\\n(.+?)\\n', text[id_match.end():])\n",
        "        if lien_match:\n",
        "            document_info[\"Lien\"] = lien_match.group(1).strip()\n",
        "\n",
        "    return document_info\n",
        "\n",
        "# Fonction pour retourner les tokens sans ponctuation et prépositions\n",
        "def return_clean_tokens(text):\n",
        "    # Tokeniser la phrase\n",
        "    doc = nlp(text)\n",
        "    # Filtrer les tokens\n",
        "    clean_tokens = [token.text for token in doc if not token.is_punct and not token.is_stop]\n",
        "    return clean_tokens\n",
        "\n",
        "# Chemin du répertoire contenant les fichiers texte\n",
        "repertoire_documents = \"/content/ocr_results\"\n",
        "\n",
        "# Liste pour stocker les informations extraites de tous les documents\n",
        "liste_informations = []\n",
        "\n",
        "# Parcourir tous les fichiers dans le dossier\n",
        "for fichier in os.listdir(repertoire_documents):\n",
        "    chemin_fichier = os.path.join(repertoire_documents, fichier)\n",
        "    with open(chemin_fichier, \"r\", encoding=\"utf-8\") as fichier_texte:\n",
        "        contenu_fichier = fichier_texte.read()\n",
        "        document_info = extract_document_info(contenu_fichier)\n",
        "        # Ajouter le nom du fichier au dictionnaire d'informations\n",
        "        document_info[\"Nom du fichier\"] = os.path.basename(chemin_fichier)\n",
        "        # Vérifier si le titre est extrait avec succès\n",
        "        if \"Titre\" in document_info:\n",
        "            # Ajouter les mots-clés au dictionnaire d'informations\n",
        "            document_info[\"Mots cles\"] = return_clean_tokens(document_info[\"Titre\"])\n",
        "        else:\n",
        "            document_info[\"Titre\"] = \"N/A\"\n",
        "            document_info[\"Mots cles\"] = []\n",
        "        liste_informations.append(document_info)\n",
        "\n",
        "# Chemin du fichier CSV de sortie\n",
        "chemin_fichier_csv = \"/content/listinfo2.csv\"\n",
        "\n",
        "# Écrire les informations extraites dans un fichier CSV\n",
        "with open(chemin_fichier_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fichier_csv:\n",
        "    # Définir les noms des colonnes\n",
        "    colonnes = [\"ID\", \"Titre\", \"Auteurs\", \"Date de Publication\", \"Annee de Publication\", \"Lien\", \"Mots cles\", \"Nom du fichier\"]\n",
        "    writer = csv.DictWriter(fichier_csv, fieldnames=colonnes)\n",
        "    # Écrire les noms des colonnes dans le fichier CSV\n",
        "    writer.writeheader()\n",
        "    # Écrire les informations extraites pour chaque document\n",
        "    for info_document in liste_informations:\n",
        "        writer.writerow(info_document)\n",
        "\n",
        "print(\"Extraction et écriture des informations terminées.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd6qZ1oCEHaS",
        "outputId": "16ba941d-d3bb-469f-f29b-19deabafcef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction et écriture des informations terminées.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classer les documents par année de publication\n",
        "documents_tries = sorted(liste_informations, key=lambda x: x.get(\"Annee de Publication\", 0))\n",
        "\n",
        "# Afficher les documents classés\n",
        "for document in documents_tries:\n",
        "    print(\"Titre:\", document.get(\"Titre\", \"N/A\"))\n",
        "    print(\"Auteurs:\", document.get(\"Auteurs\", \"N/A\"))\n",
        "    print(\"Date de Publication:\", document.get(\"Date de Publication\", \"N/A\"))\n",
        "    print(\"Annee de Publication:\", document.get(\"Annee de Publication\", \"N/A\"))\n",
        "    print(\"Mots cles:\", document.get(\"Mots cles\", \"N/A\"))\n",
        "    print(\"Lien:\", document.get(\"Lien\", \"N/A\"))\n",
        "    print(\"Nom du fichier:\", document.get(\"Nom du fichier\", \"N/A\"))\n",
        "    print(\"----------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISoRbvNEOl_U",
        "outputId": "1ab3d14e-b8d2-4e07-bae9-e678c7aeb606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titre: \\\n",
            "Auteurs: Mathilde Mougin\n",
            "Date de Publication: 3 Mar 2023\n",
            "Annee de Publication: 1578\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-04011976\n",
            "Nom du fichier: binarized_VergerXXV - MathildeMougin_first_page.txt\n",
            "----------------------\n",
            "Titre: Boleslas Ier, surnommé Chorbry le Grand, traduction\n",
            "Auteurs: par Gérard d’un poéme de J. U. Niemcewicz, tiré de La\n",
            "Date de Publication: N/A\n",
            "Annee de Publication: 1833\n",
            "Mots cles: ['Boleslas', 'Ier', 'surnommé', 'Chorbry', 'Grand', 'traduction']\n",
            "Lien: https://univ-paris8.hal.science/hal-03691092\n",
            "Nom du fichier: binarized_46. Nerval, Boleslas Ier, Chrobry le Grand_first_page.txt\n",
            "----------------------\n",
            "Titre: \\\n",
            "Auteurs: Karim Zaouaq\n",
            "Date de Publication: N/A\n",
            "Annee de Publication: 2019\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-02860300\n",
            "Nom du fichier: binarized_ZAOUAQ_2020_first_page.txt\n",
            "----------------------\n",
            "Titre: Une question de conjonction\n",
            "Auteurs: Sylvain Dournel\n",
            "Date de Publication: 31 Jan 2024\n",
            "Annee de Publication: 2020\n",
            "Mots cles: ['question', 'conjonction']\n",
            "Lien: https://hal.science/hal-04429237\n",
            "Nom du fichier: binarized_Préface Europe_first_page.txt\n",
            "----------------------\n",
            "Titre: De ’animalisation a la neutralisation: fonctionnement\n",
            "Auteurs: des verbes de bruit associés aux animaux\n",
            "Date de Publication: 20 Apr 2021\n",
            "Annee de Publication: 2020\n",
            "Mots cles: ['animalisation', 'neutralisation', 'fonctionnement']\n",
            "Lien: https://hal.science/hal-02962702v2\n",
            "Nom du fichier: binarized_I.KOR CHAHINE_2020_De l'animalisation à la neutralisation_pdf_first_page.txt\n",
            "----------------------\n",
            "Titre: “ Fclairs analogiques. La métaphore dans L’Homme\n",
            "Auteurs: foudroyé\n",
            "Date de Publication: 31 Jan 2024\n",
            "Annee de Publication: 2020\n",
            "Mots cles: ['Fclairs', 'analogiques', 'métaphore', 'Homme']\n",
            "Lien: https://hal.science/hal-04428896\n",
            "Nom du fichier: binarized_Eclairs analogiques_first_page.txt\n",
            "----------------------\n",
            "Titre: Interaction homme-machine et personnalisation des\n",
            "Auteurs: Stéphanie Rey, Christophe Bortolaso, Anke Brock, Célia Picard, Mustapha\n",
            "Date de Publication: 6 Jul 2020\n",
            "Annee de Publication: 2020\n",
            "Mots cles: ['Interaction', 'homme-machine', 'personnalisation']\n",
            "Lien: https://enac.hal.science/hal-02883368\n",
            "Nom du fichier: binarized_REY-Culture&Musées_first_page.txt\n",
            "----------------------\n",
            "Titre: Lambeaux d’archives: pour une histoire des reliquats\n",
            "Auteurs: Marie-Jeanne Zenetti\n",
            "Date de Publication: 3 Apr 2024\n",
            "Annee de Publication: 2020\n",
            "Mots cles: ['Lambeaux', 'archives', 'histoire', 'reliquats']\n",
            "Lien: https://hal.univ-lyon2.fr /hal-04511758\n",
            "Nom du fichier: binarized_2020_Zenetti_Farge_first_page.txt\n",
            "----------------------\n",
            "Titre: \\\n",
            "Auteurs: Rajaonasy\n",
            "Date de Publication: 12 Jan 2022\n",
            "Annee de Publication: 2021\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-03522299\n",
            "Nom du fichier: binarized_Text_integralle_et_Poster_first_page.txt\n",
            "----------------------\n",
            "Titre: N/A\n",
            "Auteurs: N/A\n",
            "Date de Publication: 7 Jun 2021\n",
            "Annee de Publication: 2021\n",
            "Mots cles: []\n",
            "Lien: https://inria.hal.science/hal-03252221\n",
            "Nom du fichier: binarized_Sécurité des objets connectés _ attaquer pour mieux se défendre_1__first_page.txt\n",
            "----------------------\n",
            "Titre: Le machine learning, nouvelle porte d’entrée pour les\n",
            "Auteurs: Emilie Bout, Valeria Loscri\n",
            "Date de Publication: 23 Nov 2021\n",
            "Annee de Publication: 2021\n",
            "Mots cles: ['machine', 'learning', 'nouvelle', 'porte', 'entrée']\n",
            "Lien: https://hal.science/hal-03407926\n",
            "Nom du fichier: binarized_theconversation.com-Le machine learning nouvelle porte dentrée pour lesnbspattaquants dobjetsnbspconnectés_first_page.txt\n",
            "----------------------\n",
            "Titre: \\\n",
            "Auteurs: » To cite this version:\n",
            "Date de Publication: 28 Dec 2023\n",
            "Annee de Publication: 2021\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-04365988\n",
            "Nom du fichier: binarized_De l Ontologie philosophique aux ontologies informatiques_first_page.txt\n",
            "----------------------\n",
            "Titre: N/A\n",
            "Auteurs: N/A\n",
            "Date de Publication: 2 Sep 2023\n",
            "Annee de Publication: 2021\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-03494844\n",
            "Nom du fichier: binarized_346_Ruggia_Vanni_HAL_first_page.txt\n",
            "----------------------\n",
            "Titre: Gallica sur les réseaux sociaux numériques ou la\n",
            "Auteurs: Sophie Bertrand, Isabelle Degrange\n",
            "Date de Publication: 24 Mar 2021\n",
            "Annee de Publication: 2021\n",
            "Mots cles: ['Gallica', 'réseaux', 'sociaux', 'numériques']\n",
            "Lien: https://hal.science/hal-03170454\n",
            "Nom du fichier: binarized_balisages_2_5_bertrand_degrange_first_page.txt\n",
            "----------------------\n",
            "Titre: Traitement Automatique de Langues pour la\n",
            "Auteurs: Natalia Grabar, Anais Koptient, Rémi Cardon\n",
            "Date de Publication: 4 Jan 2022\n",
            "Annee de Publication: 2021\n",
            "Mots cles: ['Traitement', 'Automatique', 'Langues']\n",
            "Lien: https://hal.science/hal-03509652\n",
            "Nom du fichier: binarized_grabar-AFIA2021-112_first_page.txt\n",
            "----------------------\n",
            "Titre: \\\n",
            "Auteurs: Chaminade\n",
            "Date de Publication: 6 Oct 2021\n",
            "Annee de Publication: 2021\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-03349977\n",
            "Nom du fichier: binarized_TAL_corrections_first_page.txt\n",
            "----------------------\n",
            "Titre: \\\n",
            "Auteurs: Stelene Narainen\n",
            "Date de Publication: 15 Nov 2021\n",
            "Annee de Publication: 2021\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-03429054\n",
            "Nom du fichier: binarized_L’Enseignement du FLE en Face à Face et En Ligne _ deux modes d'intervention pédagogiques complémentaires_first_page.txt\n",
            "----------------------\n",
            "Titre: “ Nouvelle approche d’implémentation d’un référentiel\n",
            "Auteurs: de Systéeme d’information\n",
            "Date de Publication: 6 Oct 2021\n",
            "Annee de Publication: 2021\n",
            "Mots cles: ['Nouvelle', 'approche', 'implémentation', 'référentiel']\n",
            "Lien: https://hal.science/hal-03366980\n",
            "Nom du fichier: binarized_reférentiel SI_first_page.txt\n",
            "----------------------\n",
            "Titre: Extraction des caractéristiques lexico-grammaticales et\n",
            "Auteurs: Saint Germes Bienvenu Bengono Obiang, Norbert Tsopze\n",
            "Date de Publication: 26 Jul 2021\n",
            "Annee de Publication: 2021\n",
            "Mots cles: ['Extraction', 'caractéristiques', 'lexico', 'grammaticales']\n",
            "Lien: https://hal.science/hal-02557636v4\n",
            "Nom du fichier: binarized_Aspect_Extraction__CRI_2019_first_page.txt\n",
            "----------------------\n",
            "Titre: N/A\n",
            "Auteurs: N/A\n",
            "Date de Publication: 15 Mar 2023\n",
            "Annee de Publication: 2022\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-03917814\n",
            "Nom du fichier: binarized_tipa-5424_first_page.txt\n",
            "----------------------\n",
            "Titre: Apport des techniques dans le domaine des déficits\n",
            "Auteurs: » To cite this version:\n",
            "Date de Publication: 19 Jun 2023\n",
            "Annee de Publication: 2022\n",
            "Mots cles: ['Apport', 'techniques', 'domaine', 'déficits']\n",
            "Lien: N/A\n",
            "Nom du fichier: binarized_BeMiChaGa22_first_page.txt\n",
            "----------------------\n",
            "Titre: Signaler le mensonge dans le Roman de Renart. Jeux et\n",
            "Auteurs: Valentine Eugéne\n",
            "Date de Publication: 8 Feb 2023\n",
            "Annee de Publication: 2022\n",
            "Mots cles: ['Signaler', 'mensonge', 'Roman', 'Renart', 'Jeux']\n",
            "Lien: https://hal.sorbonne-universite.fr/hal-03978844\n",
            "Nom du fichier: binarized_article pdf perspectives médiévales signaler le mensonge_first_page.txt\n",
            "----------------------\n",
            "Titre: Notes de lecture: a propos de Carine Capone, Aux\n",
            "Auteurs: Sylvain Dournel\n",
            "Date de Publication: 5 Feb 2024\n",
            "Annee de Publication: 2023\n",
            "Mots cles: ['Notes', 'lecture', 'propos', 'Carine', 'Capone']\n",
            "Lien: https://hal.science/hal-04439035\n",
            "Nom du fichier: binarized_CR évènement_first_page.txt\n",
            "----------------------\n",
            "Titre: \\\n",
            "Auteurs: » To cite this version:\n",
            "Date de Publication: 18 Jan 2024\n",
            "Annee de Publication: 2023\n",
            "Mots cles: []\n",
            "Lien: https://inria.hal.science/hal-04401882\n",
            "Nom du fichier: binarized_Bouchabou2023RODA_first_page.txt\n",
            "----------------------\n",
            "Titre: Roles et actions d’un chargé de mission numérique\n",
            "Auteurs: Thierry Oger\n",
            "Date de Publication: 11 Feb 2024\n",
            "Annee de Publication: 2023\n",
            "Mots cles: ['Roles', 'actions', 'chargé', 'mission', 'numérique']\n",
            "Lien: https://hal.science/hal-04451140\n",
            "Nom du fichier: binarized_amue-collection-numerique-29_p56-57_first_page.txt\n",
            "----------------------\n",
            "Titre: \\\n",
            "Auteurs: Marine Lambolez\n",
            "Date de Publication: 14 Feb 2024\n",
            "Annee de Publication: 2023\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-04457857\n",
            "Nom du fichier: binarized_57c4073da94c4d6ec4afe7ca1b86a341_first_page.txt\n",
            "----------------------\n",
            "Titre: \\\n",
            "Auteurs: » To cite this version:\n",
            "Date de Publication: 28 Oct 2023\n",
            "Annee de Publication: 2023\n",
            "Mots cles: []\n",
            "Lien: https://hal.science/hal-04263624\n",
            "Nom du fichier: binarized_Temporary Event Urbanism_first_page.txt\n",
            "----------------------\n",
            "Titre: Les objets connectés pour (presque) tous\n",
            "Auteurs: Matthieu Gautier, Olivier Berder, Claire Le Page\n",
            "Date de Publication: 18 Jan 2024\n",
            "Annee de Publication: 2023\n",
            "Mots cles: ['objets', 'connectés', 'presque']\n",
            "Lien: https://inria.hal.science/hal-04401963\n",
            "Nom du fichier: binarized_gautier2023j3ea_first_page.txt\n",
            "----------------------\n",
            "Titre: Créativité assistée par ordinateur: composer la musique\n",
            "Auteurs: Felipe Ariani, Marcelo Caetano, Javier Elipe Gimeno, Ivan\n",
            "Date de Publication: 16 Feb 2023\n",
            "Annee de Publication: 2023\n",
            "Mots cles: ['Créativité', 'assistée', 'ordinateur', 'composer', 'musique']\n",
            "Lien: https://hal.science/hal-03992730\n",
            "Nom du fichier: binarized_Créativité assistée par ordinateur composer la musique d'un film final HAL_first_page.txt\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lknf7TMyWI8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}